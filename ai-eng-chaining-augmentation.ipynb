{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/B+I1W0+hKGADcxs5YL6M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We've actually already seen how it can be useful to \"chain\" various LLM operations together. In the Hinglish chat example we chained a response generation and then a machine translation using LLMs.\n","\n","**As you solve problems with LLMs, do NOT always think about your task as a single prompt.** Decompose your problem into multiple steps. Just like programming which uses multiple functions, classes, etc. LLM integration is a new kind of reasoning engine that you can \"program\" in a multi-step, conditional, control flow sort of fashion.\n","\n","Further, enterprise LLM appllications need reliability, trust, and consistency. **Because LLMs only predict probable text, they have no understanding or connection to reality.** This produces **hallucinations** that can be part of a coherent text block but factually (or otherwise) wrong. To deal with this we need to **ground** on LLM operations with external data."],"metadata":{"id":"xm6EIVbldZer"}},{"cell_type":"markdown","source":["# Dependencies and imports"],"metadata":{"id":"wK6zNdTOdVcE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2yX1z4WyJP2"},"outputs":[],"source":["! pip install langchain predictionguard lancedb html2text sentence-transformers"]},{"cell_type":"code","source":["import os\n","import urllib.request\n","\n","import html2text\n","import predictionguard as pg\n","from langchain import PromptTemplate, FewShotPromptTemplate\n","from langchain.text_splitter import CharacterTextSplitter\n","from sentence_transformers import SentenceTransformer\n","import numpy as np\n","from getpass import getpass\n","import lancedb\n","from lancedb.embeddings import with_embeddings\n","import pandas as pd"],"metadata":{"id":"Go5vRQcTycUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg_access_token = getpass('Enter your Prediction Guard access token: ')\n","os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"],"metadata":{"id":"HdSloPn7JTu0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chaining"],"metadata":{"id":"eTH1n7VuNm5g"}},{"cell_type":"code","source":["template = \"\"\"### Instruction:\n","Decide if the following input message is an informational question, a general chat message, or a request for code generation.\n","If the message is an informational question, answer it based on the informational context provided below.\n","If the message is a general chat message, respond in a kind and friendly manner based on the coversation context provided below.\n","If the message is a request for code generation, respond with a code snippet.\n","\n","### Input:\n","Message: {query}\n","\n","Informational Context: The Greater Los Angeles and San Francisco Bay areas in California are the nation's second and fifth-most populous urban regions, respectively. Greater Los Angeles has over 18.7 million residents and the San Francisco Bay Area has over 9.6 million residents. Los Angeles is state's most populous city and the nation's second-most populous city. San Francisco is the second-most densely populated major city in the country. Los Angeles County is the country's most populous county, and San Bernardino County is the nation's largest county by area. Sacramento is the state's capital.\n","\n","Conversational Context:\n","Human - \"Hello, how are you?\"\n","AI - \"I'm good, what can I help you with?\"\n","Human - \"What is the captital of California?\"\n","AI - \"Sacramento\"\n","Human - \"Thanks!\"\n","AI - \"You are welcome!\"\n","\n","### Response:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=template,\n",")"],"metadata":{"id":"wiWYZO_xNr7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pg.Completion.create(\n","    model=\"Nous-Hermes-Llama2-13B\",\n","    prompt=prompt.format(query=\"What is the population of LA?\")\n",")\n","\n","print(result['choices'][0]['text'])"],"metadata":{"id":"bpXQfN0uk59t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["category_template = \"\"\"### Instruction:\n","Read the below input and determine if it is a request to generate computer code? Respond \"yes\" or \"no\".\n","\n","### Input:\n","{query}\n","\n","### Response:\n","\"\"\"\n","\n","category_prompt = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=category_template\n",")\n","\n","qa_template = \"\"\"### Instruction:\n","Read the context below and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","### Input:\n","Context: {context}\n","\n","Question: {query}\n","\n","### Response:\n","\"\"\"\n","\n","qa_prompt = PromptTemplate(\n","    input_variables=[\"context\", \"query\"],\n","    template=qa_template\n",")\n","\n","chat_template = \"\"\"### Instruction:\n","You are a friendly and clever AI assistant. Respond to the latest human message in the input conversation below.\n","\n","### Input:\n","{context}\n","Human: {query}\n","AI:\n","\n","### Response:\n","\"\"\"\n","\n","chat_prompt = PromptTemplate(\n","    input_variables=[\"context\", \"query\"],\n","    template=chat_template\n",")\n","\n","code_template = \"\"\"### Instruction:\n","You are a code generation assistant. Respond with a code snippet and any explanation requested in the below input.\n","\n","### Input:\n","{query}\n","\n","### Response:\n","\"\"\"\n","\n","code_prompt = PromptTemplate(\n","    input_variables=[\"query\"],\n","    template=code_template\n",")\n","\n","\n","# QuestionID provides some help in determining if a sentence is a question.\n","class QuestionID:\n","    \"\"\"\n","        QuestionID has the actual logic used to determine if sentence is a question\n","    \"\"\"\n","    def padCharacter(self, character: str, sentence: str):\n","        if character in sentence:\n","            position = sentence.index(character)\n","            if position > 0 and position < len(sentence):\n","\n","                # Check for existing white space before the special character.\n","                if (sentence[position - 1]) != \" \":\n","                    sentence = sentence.replace(character, (\" \" + character))\n","\n","        return sentence\n","\n","    def predict(self, sentence: str):\n","        questionStarters = [\n","            \"which\", \"wont\", \"cant\", \"isnt\", \"arent\", \"is\", \"do\", \"does\",\n","            \"will\", \"can\"\n","        ]\n","        questionElements = [\n","            \"who\", \"what\", \"when\", \"where\", \"why\", \"how\", \"sup\", \"?\"\n","        ]\n","\n","        sentence = sentence.lower()\n","        sentence = sentence.replace(\"\\'\", \"\")\n","        sentence = self.padCharacter('?', sentence)\n","        splitWords = sentence.split()\n","\n","        if any(word == splitWords[0] for word in questionStarters) or any(\n","                word in splitWords for word in questionElements):\n","            return True\n","        else:\n","            return False\n","\n","def response_chain(message, convo_context, info_context):\n","\n","  # Determine what kind of message this is.\n","  result = pg.Completion.create(\n","      model=\"WizardCoder\",\n","      prompt=category_prompt.format(query=message),\n","      output={\n","          \"type\": \"categorical\",\n","          \"categories\": [\"yes\", \"no\"]\n","      }\n","  )\n","\n","  # configure our chain\n","  code = result['choices'][0]['output']\n","  qIDModel = QuestionID()\n","  question = qIDModel.predict(message)\n","\n","  if code == \"no\" and question:\n","\n","    # Handle the informational request.\n","    result = pg.Completion.create(\n","        model=\"Nous-Hermes-Llama2-13B\",\n","        prompt=qa_prompt.format(context=info_context, query=message)\n","    )\n","    completion = result['choices'][0]['text'].split('#')[0].strip()\n","\n","  elif code == \"yes\":\n","\n","    # Handle the code generation request.\n","    result = pg.Completion.create(\n","        model=\"WizardCoder\",\n","        prompt=code_prompt.format(query=message),\n","        max_tokens=500\n","    )\n","    completion = result['choices'][0]['text']\n","\n","  else:\n","\n","    # Handle the chat message.\n","    result = pg.Completion.create(\n","        model=\"Nous-Hermes-Llama2-13B\",\n","        prompt=chat_prompt.format(context=convo_context, query=message),\n","        output={\n","            \"toxicity\": True\n","        }\n","    )\n","    completion = result['choices'][0]['text'].split('Human:')[0].strip()\n","\n","  return code, question, completion\n"],"metadata":{"id":"82oTOlF6l0-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["info_context = \"The Greater Los Angeles and San Francisco Bay areas in California are the nation's second and fifth-most populous urban regions, respectively. Greater Los Angeles has over 18.7 million residents and the San Francisco Bay Area has over 9.6 million residents. Los Angeles is state's most populous city and the nation's second-most populous city. San Francisco is the second-most densely populated major city in the country. Los Angeles County is the country's most populous county, and San Bernardino County is the nation's largest county by area. Sacramento is the state's capital.\"\n","\n","convo_context = \"\"\"Human: Hello, how are you?\n","AI: I'm good, what can I help you with?\n","Human: What is the captital of California?\n","AI: Sacramento\n","Human: Thanks!\n","AI: You are welcome!\"\"\"\n","\n","message = \"Which city in California has the highest population?\"\n","#message = \"I'm really enjoying this conversation.\"\n","#message = \"Generate some python code that gets the current weather in the bay area.\"\n","\n","code, question, completion = response_chain(message, convo_context, info_context)\n","print(\"CODE GEN REQUESTED:\", code)\n","print(\"QUESTION:\", question)\n","print(\"\")\n","print(\"RESPONSE:\", completion)"],"metadata":{"id":"yWM2HeVNpFNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# External knowledge in prompts, Grounding"],"metadata":{"id":"EMW-amKXI-nk"}},{"cell_type":"markdown","source":["We've actually already seen external knowledge within our prompts. In the question and answer example, the `context` that we pasted in was a copy of phrasing on the Domino's website."],"metadata":{"id":"UBxGhPzLebrA"}},{"cell_type":"code","source":["template = \"\"\"### Instruction:\n","Read the context below and respond with an answer to the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","### Input:\n","Context: {context}\n","\n","Question: {question}\n","\n","### Response:\n","\"\"\"\n","\n","prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")"],"metadata":{"id":"vkmyGTHuJEc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["context = \"Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\"\n","\n","question = \"How are gift cards delivered?\"\n","\n","myprompt = prompt.format(context=context, question=question)\n","print(myprompt)"],"metadata":{"id":"XRkwJGTOJIAC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pg.Completion.create(\n","    model=\"Nous-Hermes-Llama2-13B\",\n","    prompt=myprompt\n",")\n","result['choices'][0]['text'].split('#')[0].strip()"],"metadata":{"id":"sbyyGvyjJYCN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Retrieval augmentation"],"metadata":{"id":"rEYisXxsynLc"}},{"cell_type":"markdown","source":["Retrieval-augmented generation (RAG) is an innovative approach that merges the capabilities of large-scale retrieval systems with sequence-to-sequence models to enhance their performance in generating detailed and contextually relevant responses. Instead of relying solely on the knowledge contained within the model's parameters, RAG allows the model to dynamically retrieve and integrate information from an external database or a set of documents during the generation process. By doing so, it provides a bridge between the vast knowledge stored in external sources and the powerful generation abilities of neural models, enabling more informed, diverse, and context-aware outputs in tasks like question answering, dialogue systems, and more."],"metadata":{"id":"3ysqzdilA62F"}},{"cell_type":"code","source":["# Let's get the html off of a website.\n","fp = urllib.request.urlopen(\"https://docs.kernel.org/process/submitting-patches.html\")\n","mybytes = fp.read()\n","html = mybytes.decode(\"utf8\")\n","fp.close()\n","\n","# And convert it to text.\n","h = html2text.HTML2Text()\n","h.ignore_links = True\n","text = h.handle(html)\n","\n","print(text)"],"metadata":{"id":"c0AfYyA7vcrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clean things up just a bit.\n","text = text.split(\"### This Page\")[1]\n","text = text.split(\"## References\")[0]\n","print(text)"],"metadata":{"id":"AnH3LL-owZOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chunk the text into smaller pieces for injection into LLM prompts.\n","text_splitter = CharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n","docs = text_splitter.split_text(text)\n","len(docs)"],"metadata":{"id":"KjNBPI_Fwv5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's checkout some of the chunks!\n","for i in range(0, 3):\n","  print(\"Chunk\", str(i+1))\n","  print(\"----------------------------\")\n","  print(docs[i])\n","  print(\"\")"],"metadata":{"id":"XPgwoO2w4-Lr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's take care of some of the formatting so it doesn't conflict with our\n","# typical prompt template structure\n","docs = [x.replace('#', '-') for x in docs]\n","print(docs[2])"],"metadata":{"id":"Sl5ib7UU5dKd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now we need to embed these documents and put them into a \"vector store\" or\n","# \"vector db\" that we will use for semantic search and retrieval.\n","\n","# Embeddings setup\n","name=\"all-MiniLM-L12-v2\"\n","model = SentenceTransformer(name)\n","\n","def embed_batch(batch):\n","    return [model.encode(sentence) for sentence in batch]\n","\n","def embed(sentence):\n","    return model.encode(sentence)\n","\n","# LanceDB setup\n","os.mkdir(\".lancedb\")\n","uri = \".lancedb\"\n","db = lancedb.connect(uri)\n","\n","# Create a dataframe with the chunk ids and chunks\n","metadata = []\n","for i in range(len(docs)):\n","    metadata.append([\n","        i,\n","        docs[i]\n","    ])\n","doc_df = pd.DataFrame(metadata, columns=[\"chunk\", \"text\"])\n","doc_df.head()"],"metadata":{"id":"-rsGYSGR5w95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embed the documents\n","data = with_embeddings(embed_batch, doc_df)\n","data.to_pandas().head()"],"metadata":{"id":"JgsIA1un7Pyq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the DB table and add the records.\n","db.create_table(\"linux\", data=data)\n","table = db.open_table(\"linux\")\n","table.add(data=data)"],"metadata":{"id":"F022vhos7hth"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Let's try to match a query to one of our documents.\n","message = \"How many problems should be solved per patch?\"\n","results = table.search(embed(message)).limit(5).to_df()\n","results.head()"],"metadata":{"id":"daHLa4oH7q10"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Now let's augment our Q&A prompt with this external knowledge on-the-fly!!!\n","template = \"\"\"### Instruction:\n","Read the below input context and respond with a short answer to the given question. Use only the information in the below input to answer the question. If you cannot answer the question, respond with \"Sorry, I can't find an answer, but you might try looking in the following resource.\"\n","\n","### Input:\n","Context: {context}\n","\n","Question: {question}\n","\n","### Response:\n","\"\"\"\n","qa_prompt = PromptTemplate(\n","    input_variables=[\"context\", \"question\"],\n","    template=template,\n",")\n","\n","def rag_answer(message):\n","\n","  # Search the for relevant context\n","  results = table.search(embed(message)).limit(5).to_df()\n","  results.sort_values(by=['_distance'], inplace=True, ascending=True)\n","  doc_use = results['text'].values[0]\n","\n","  # Augment the prompt with the context\n","  prompt = qa_prompt.format(context=doc_use, question=message)\n","\n","  # Get a response\n","  result = pg.Completion.create(\n","      model=\"Nous-Hermes-Llama2-13B\",\n","      prompt=prompt\n","  )\n","\n","  return result['choices'][0]['text']\n"],"metadata":{"id":"IDscT_858T4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = rag_answer(\"How many problems should be solved in a single patch?\")\n","\n","print('')\n","print(\"RESPONSE:\", response)"],"metadata":{"id":"pRo91E2w9adt"},"execution_count":null,"outputs":[]}]}