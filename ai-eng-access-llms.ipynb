{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMNrg1I2PYN0rsnXtweI9oG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["To run any prompt through a model, we need to set a foundation for how we will access generative AI models and perform inference. There is a huge variety in the landscape of generative AI models in terms of size, access patterns, licensing, etc. However, a common theme is the usage of LLMs through a REST API, which is either:\n","- Provided by a third party service (OpenAI, Anthropic, Cohere, etc.)\n","- Self-hosted in your own infrastructure or in an account you control with a model hosting provider (Replicate, Baseten, etc.)\n","- Self-hosted using a DIY model serving API (Flask, FastAPI, etc.)\n","\n","We will use a tool called [Prediction Guard](https://www.predictionguard.com/) to call both proprietary models (like OpenAI) and open access LLMs (like Llama 2, WizardCoder, MPT, etc.) via a standardized OpenAI-like API. This will allow us to explore the full range of LLMs available. Further, it will illustrate how companies can access a wide range of models (outside of the GPT family).\n","\n","If you are interested, Prediction Guard does provide some significant functionality on top of this standardized API (see the [docs](https://docs.predictionguard.com/)). Specifically, it lets you:\n","\n","- **Control** the structure of and easily constrain LLM output to the types, formats, and information relevant to your business;\n","- **Validate** and check LLM output to guard against hallucination and toxicity; and\n","- **Implement compliant LLM systems** (HIPAA, and self-hosted) that give your legal counsel warm fuzzy feeling while still delighting your customers with AI features.\n","\n","To run your first LLM prompt with *Prediction Guard*, you will need a Prediction Guard access token that will be provided to you by the instructor."],"metadata":{"id":"tx0f1rKRWqS_"}},{"cell_type":"markdown","source":["# Install dependences, imports"],"metadata":{"id":"ZGe8RF_LzKjK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pI0jTm47xNj5"},"outputs":[],"source":["! pip install predictionguard"]},{"cell_type":"code","source":["import os\n","import json\n","\n","import predictionguard as pg\n","from getpass import getpass"],"metadata":{"id":"Wg7xvnBhxb38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg_access_token = getpass('Enter your Prediction Guard access token: ')\n","os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"],"metadata":{"id":"K_cUA6tClxcM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# List available models"],"metadata":{"id":"eutZBE8vtLC5"}},{"cell_type":"markdown","source":["You can find out more about the models available via the Prediction Guard API [in the docs](https://docs.predictionguard.com/models)."],"metadata":{"id":"obLO0rEGtPTE"}},{"cell_type":"code","source":["pg.Completion.list_models()"],"metadata":{"id":"wM5pESLxtXic"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate some text from the latest open access LLMs"],"metadata":{"id":"fr7dHK-VyW2s"}},{"cell_type":"code","source":["response = pg.Completion.create(model=\"Nous-Hermes-Llama2-13B\",\n","                          prompt=\"The best joke I know is: \")\n","\n","print(json.dumps(\n","    response,\n","    sort_keys=True,\n","    indent=4,\n","    separators=(',', ': ')\n","))"],"metadata":{"id":"9xw7U9qDzPKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate text from a proprietary LLM (OpenAI)"],"metadata":{"id":"54ybrbDxnSi6"}},{"cell_type":"code","source":["openai_api_key = getpass('Enter your OpenAI API key: ')\n","os.environ['OPENAI_API_KEY'] = openai_api_key"],"metadata":{"id":"JadVmYcYnxj-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = pg.Completion.create(model=\"OpenAI-text-davinci-003\",\n","                          prompt=\"The best joke I know is: \")\n","\n","print(response['choices'][0]['text'])"],"metadata":{"id":"cwAXEVuHnVUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-jeL4vF3xSui"},"execution_count":null,"outputs":[]}]}