{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOx0Sg7DZAA6IGGWs1IfFty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Prompting** is the process of providing a partial, usually text, input to a model. As we discussed in the last chapter, models will then use their parameterized data transformations to find a probable completion or output that matches the prompt.\n","\n","**Prompt Engineering** is the emerging developer task of designing and optimizing prompts for AI models to achieve specific goals or outcomes. It involves creating high-quality inputs that can elicit accurate and relevant responses from AI models. The next several examples will help get you up to speed on common prompt engineering strategies.\n","\n","```\n","               +-------------------+\n","               |                   |\n","               |                   |  Completion\n","Prompt         |       Large       |  Generated text\n","--------------->     Language      +------------->\n","               |       Model       |\n","               |       (LLM)       |\n","               |                   |\n","               +-------------------+\n","```"],"metadata":{"id":"o14TvGScYAqZ"}},{"cell_type":"markdown","source":["# Dependencies and imports"],"metadata":{"id":"Jc-nVEbsX8bU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"u9pot_Yc2FMw"},"outputs":[],"source":["! pip install predictionguard"]},{"cell_type":"code","source":["import os\n","\n","import predictionguard as pg\n","from getpass import getpass"],"metadata":{"id":"rOVhsPn42JEl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg_access_token = getpass('Enter your Prediction Guard access token: ')\n","os.environ['PREDICTIONGUARD_TOKEN'] = pg_access_token"],"metadata":{"id":"l8sDezef2Me8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Autocomplete"],"metadata":{"id":"haoOqKSw2azm"}},{"cell_type":"markdown","source":["Because LLMs are configured/ trained to perform the task of text completion, the most basic kind of prompt that you might provide is an **autocomplete** prompt. Regardless of prompt structure, the model function will compute the probabilities of words, tokens, or characters that might follow in the sequence of words, tokens, or characters that you provided in the prompt.\n","\n","Depending on the desired outcome, the prompt may be a single sentence, a paragraph, or even an partial story. Additionally, the prompt may be open-ended, providing only a general topic or theme, or it may be more specific, outlining a particular scenario or plot."],"metadata":{"id":"BInWcXfcYd0M"}},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=\"Daniel Whitenack, a long forgotten wizard from the Lord of the Rings, entered into Mordor to\"\n",")['choices'][0]['text']"],"metadata":{"id":"9EBTZ6-V2dpo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=\"Today I inspected the engine mounting equipment. I found a problem in one of the brackets so\"\n",")['choices'][0]['text']"],"metadata":{"id":"13GECBS2M-Aa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=\"\"\"CREATE TABLE llm_queries(id SERIAL PRIMARY KEY, name TEXT NOT NULL, value REAL);\n","INSERT INTO llm_queries('Daniel Whitenack', 'autocomplete')\n","SELECT\"\"\"\n",")['choices'][0]['text']"],"metadata":{"id":"RSoTsZtGNHwR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Zero shot prompts"],"metadata":{"id":"oZvw-mv52TL6"}},{"cell_type":"markdown","source":["Autocomplete is a great place to start, but it is only that: a place to start. Throughout this workshop we will be putting on our prompt engineering hats to do some mind blowing things with generative AI. As we continue along that path, there is a general prompt structure that will pop up over and over again:\n","\n","```\n"," Prompt:\n","+------------------------------------------------------------+\n","|                                                            |\n","|  +-------------------------------------------------------+ |\n","|  | ----------------------------------------------------- | | Task Descrip./\n","|  | ---------------------------------------               | | Instructions\n","|  +-------------------------------------------------------+ |\n","|                                                            |\n","|  +-------------------------------------------------------+ | Current Input/\n","|  | -------------                                         | | Context\n","|  +-------------------------------------------------------+ |\n","|                                                            |\n","|  +----------------------------------------+                | Output\n","|  | --------------------------             |                | Indicator\n","|  +----------------------------------------+                |\n","|                                                            |\n","+------------------------------------------------------------+\n","```\n","\n","One of the easiest ways to leverage the above prompt structure is to describe a task (e.g., sentiment analysis), provide a single piece of data as context, and then provide a single output indicator. This is called a **zero shot prompt**."],"metadata":{"id":"gmXVIVBoYsGJ"}},{"cell_type":"markdown","source":["## Sentiment analysis"],"metadata":{"id":"7yekZsMFNoxF"}},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=\"\"\"Assign a sentiment label to the text included below. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\n","\n","Text: This workshop is spectacular. I love it! So wonderful.\n","Sentiment:\"\"\"\n",")['choices'][0]['text']"],"metadata":{"id":"lpBfvmQp2Ryu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Question Answering"],"metadata":{"id":"5ctMfmojNz_S"}},{"cell_type":"code","source":["prompt = \"\"\"Read the context below and answer the question. If the question cannot be answered based on the context alone or the context does not explicitly say the answer to the question, write \"Sorry I had trouble answering this question, based on the information I found.\"\n","\n","Context: Domino's gift cards are great for any person and any occasion. There are a number of different options to choose from. Each comes with a personalized card carrier and is delivered via US Mail.\n","\n","Question: How are gift cards delivered?\n","\n","Answer: \"\"\""],"metadata":{"id":"kh4jlFH1NvUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"5e9ab2RiN7Js"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fraud Detection"],"metadata":{"id":"u5uz0wnfME6p"}},{"cell_type":"code","source":["prompt = \"Assign a fraud score to a customer of a life insurance company. \"\n","prompt = prompt + \"The score ranges from 0 to 1. Scores close to zero represent that the customer is very unlikely to be committing fraud. Scores close to 1 represent that a customer is very likely to be committing fraud. \"\n","prompt = prompt + \"Transaction history data includes transaction dates, category, type of insurance product, reason code, reward amount, coverage limit, and income at the time of the transaction.\\n\\n\"\n","prompt = prompt + \"\"\"Transaction History:\n","1. Date: 9/10/1977; Category: IN; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 50000.0; Income: 61000.0\n","2. Date: 12/31/2005; Category: CL; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 50000.0; Income: 61000.0\n","3. Date: 1/17/2006; Category: RE; Type: T; Reason Code: 265.0; Reward Amount: 50000.0; Coverage Limit: NaN; Income: NaN\n","4. Date: 12/15/1998; Category: IN; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 100000.0; Income: 29000.0\n","5. Date: 6/7/1961; Category: IN; Type: V; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 100000.0; Income: 48000.0\\n\\n\"\"\"\n","prompt = prompt + \"Fraud score: \"\n","\n","print(prompt)"],"metadata":{"id":"ic9qo4pOMHYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"wkzcBIZ2MIWl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"Assign a fraud score to a customer of a life insurance company. \"\n","prompt = prompt + \"The score ranges from 0 to 1. Scores close to zero represent that the customer is very unlikely to be committing fraud. Scores close to 1 represent that a customer is very likely to be committing fraud. \"\n","prompt = prompt + \"Transaction history data includes transaction dates, category, type of insurance product, reason code, reward amount, coverage limit, and income at the time of the transaction.\\n\\n\"\n","prompt = prompt + \"\"\"Statistics of all customer transactions:\n","Time between claim and reward: average is 23 days, standard deviation is 77 days\n","Coverage to income ratio: average is 7.2, standard deviation is 4.2\n","Recency according to an exponential distribution: average is 0.42, standard deviation is 0.24\n","Reward amount: average is $359,556, standard deviation $260,000\\n\\n\"\"\"\n","prompt = prompt + \"\"\"Customer transaction history:\n","1. Date: 9/10/1977; Category: IN; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 50000.0; Income: 61000.0\n","2. Date: 12/31/2005; Category: CL; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 50000.0; Income: 61000.0\n","3. Date: 1/17/2006; Category: RE; Type: T; Reason Code: 265.0; Reward Amount: 50000.0; Coverage Limit: NaN; Income: NaN\n","4. Date: 12/15/1998; Category: IN; Type: T; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 100000.0; Income: 29000.0\n","5. Date: 6/7/1961; Category: IN; Type: V; Reason Code: NaN; Reward Amount: NaN; Coverage Limit: 100000.0; Income: 48000.0\\n\\n\"\"\"\n","prompt = prompt + \"Fraud score: \"\n","\n","print(prompt)"],"metadata":{"id":"9hSLy6gBMIh2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"243ywn_xMIoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Few Shot prompts"],"metadata":{"id":"PlGYdHDy2XD1"}},{"cell_type":"markdown","source":["When your task is slightly more complicated or requires a few more leaps in reasoning to generate an appropriate response, you can turn to **few shot** prompting (aka **in context learning**). In few shot prompting, a small number of gold standard demonstrations are integrated into the prompt. These demonstrations serve as example (context, output) pairs for the model, which serve to tune the probable output on-the-fly to what we ideally want in the output.\n","\n","Although not always necessary (as seen above), few shot prompting generally produces better results than single shot prompting in terms of consistency and similarity to your ideal outputs. This does come at a cost for some models that might charge based on the number of characters or words that are input to the model API."],"metadata":{"id":"Z163pVfDZJIW"}},{"cell_type":"markdown","source":["## Sentiment"],"metadata":{"id":"_DjiAH9XOBDu"}},{"cell_type":"code","source":["prompt = \"\"\"Classify the sentiment of the text. Use the label NEU for neutral sentiment, NEG for negative sentiment, and POS for positive sentiment.\n","\n","Text: That pilot is adorable.\n","Sentiment: POS\n","\n","Text: This was an awful seat.\n","Sentiment: NEG\n","\n","Text: This pilot was brilliant.\n","Sentiment: POS\n","\n","Text: I saw the aircraft.\n","Sentiment: NEU\n","\n","Text: That food was exceptional.\n","Sentiment: POS\n","\n","Text: That was a private aircraft.\n","Sentiment: NEU\n","\n","Text: This is an unhappy pilot.\n","Sentiment: NEG\n","\n","Text: The staff is rough.\n","Sentiment: NEG\n","\n","Text: This staff is Australian.\n","Sentiment: NEU\n","\n","Text: The flight is boring.\n","Sentiment:\"\"\""],"metadata":{"id":"S7mwe3BP2egC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"Falcon-7B-Instruct\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"WcY7yVhQOJO1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Logic representations"],"metadata":{"id":"JqM6BzkHOOAr"}},{"cell_type":"code","source":["openai_access_token = getpass('Enter your OpenAI API key: ')\n","os.environ['OPENAI_API_KEY'] = openai_access_token"],"metadata":{"id":"igAXsgaVv-_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"Transform text statements into first order logic.\n","\n","Statement: Pluto is a dog\n","Logic: (dog(Pluto))\n","\n","Statement: Deb is not tall\n","Logic: (¬tall(Deb))\n","\n","Statement: Not every person who plays football knows cricket\n","Logic: (¬∀(x)(((person(x))∧(plays(x,football)))→(knows(x,cricket))))\n","\n","Statement: Some people can sell any product\n","Logic: ∃(x)((people(x))∧∀(y)((product(y))→(sell(x,y))))\n","\n","Statement: If you enroll in the course and prepare hard then you will pass the course\n","Logic: (((enroll(you,course))∧(prepare-hard(you)))→(pass(you,course)))\n","\n","Statement: Do not forget to provide all of your demonstrations\n","Logic:\"\"\""],"metadata":{"id":"TTrfCdOaOK-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pg.Completion.create(model=\"OpenAI-text-davinci-003\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"l3n6MKRvOVOW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Hinglish conversation"],"metadata":{"id":"EN2mxlUGOabe"}},{"cell_type":"code","source":["prompt = \"\"\"The following is a conversation with an AI assistant in Hinglish. The assistant is helpful, creative, clever, and very friendly. The AI assistant responds in Hinglish.\n","\n","Human: Hello\n","AI: Hello!\n","\n","Human: Movie ke baare mein tumhara initial opinion kya hai? Zyada depth mein gaye bina? Tumne like kiya ki nahin?\n","AI: Overall, mujhe movie achchi lagi.  Usme kuh high-profile actors hain,  Tum kya kahte ho?\n","\n","Human: Tumhe lagta hai ki ek achche movie mein high-profile actors hone chahiye? Ya phir, ek movie ko achcha kahne ke liye high-profile actors hone chahiye?\n","AI: Zaroori nahin, par help zaroor milta hai.  Ek actor jab tak apna kaam theek se nahin karta, tab tak high-profile nahin ban sakta. Tumhe kya lagta hai?\n","\n","Human: muje lgta hai ki yeh kewal personal pasand ke bare mai hai meri pasandida american comedy films Will Ferrol or Seth Rogan ke sath kuch bhi samil slapstick films hai muje yakin hai ki ek film thi jisme Lindsay Lohan ko bhot bada bna diya tha bhale hi usne apne surati saalo mai movies mai acting ki thi\n","AI: \"\"\""],"metadata":{"id":"fZLp1dhBOXOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output = pg.Completion.create(model=\"OpenAI-text-davinci-003\",\n","    prompt=prompt\n",")['choices'][0]['text']\n","output"],"metadata":{"id":"tCzKXV8zOl0q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"Translate the following text from Hinglish to English.\n","\n","Text: {hinglish}\n","Translation: \"\"\".format(hinglish=output)\n","\n","pg.Completion.create(model=\"OpenAI-text-davinci-003\",\n","    prompt=prompt\n",")['choices'][0]['text']"],"metadata":{"id":"gntWOt7YOnBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gP6yJn1IvTrb"},"execution_count":null,"outputs":[]}]}